import os
import faiss
import numpy as np
import google.generativeai as genai
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ‚úÖ Load Gemini API Key
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyCoZDwqGbCxFB-YY5L426MdaabNOXFbE0w")
if not GEMINI_API_KEY:
    raise ValueError("‚ùå GEMINI_API_KEY is not set!")

genai.configure(api_key="AIzaSyCoZDwqGbCxFB-YY5L426MdaabNOXFbE0w")

app = FastAPI()

# ‚úÖ Enable CORS for frontend requests
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Path to the documentation file
DOC_PATH = r"D:\final-year-project\frontend\src\components\BlockShare_Documentation.md"

def load_markdown_document():
    """Reads the Markdown file containing BlockShare documentation."""
    if not os.path.exists(DOC_PATH):
        raise FileNotFoundError(f"‚ùå Documentation file not found at {DOC_PATH}")
    
    with open(DOC_PATH, "r", encoding="utf-8") as file:
        return file.read()

def get_gemini_embedding(text):
    """Generates embeddings using Google Gemini API."""
    try:
        response = genai.embed_content(
            model="models/embedding-001",
            content=text,
            task_type="retrieval_document"
        )
        return np.array(response["embedding"])
    except Exception as e:
        raise RuntimeError(f"‚ùå Failed to get embedding: {str(e)}")

def chunk_text(text, chunk_size=300):
    """Splits text into smaller chunks for embedding."""
    sentences = text.split("\n")
    chunks, current_chunk = [], ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) < chunk_size:
            current_chunk += sentence + " "
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + " "

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

def create_faiss_index():
    """Creates a FAISS index from the documentation text."""
    print("üìÑ Loading documentation...")
    content = load_markdown_document()
    
    print("‚úÇÔ∏è Chunking text...")
    chunks = chunk_text(content)
    
    print("üîÑ Generating embeddings...")
    embeddings = [get_gemini_embedding(chunk) for chunk in chunks]
    if not embeddings:
        raise RuntimeError("‚ùå No embeddings were generated.")

    embeddings = np.array(embeddings)
    
    print("üóÑÔ∏è Building FAISS index...")
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)

    return index, chunks

# ‚úÖ Initialize FAISS index
try:
    faiss_index, text_chunks = create_faiss_index()
    print("‚úÖ FAISS index created successfully!")
except Exception as e:
    print(f"‚ùå Error initializing FAISS: {str(e)}")
    faiss_index, text_chunks = None, None

def search_faiss(query, top_k=3):
    """Searches for the most relevant chunks using FAISS."""
    if faiss_index is None:
        raise RuntimeError("‚ùå FAISS index is not initialized.")
    
    query_embedding = get_gemini_embedding(query).reshape(1, -1)
    _, closest_idx = faiss_index.search(query_embedding, k=min(top_k, len(text_chunks)))

    retrieved_text = " ".join([text_chunks[i] for i in closest_idx[0]])  # Combine retrieved chunks
    return retrieved_text

# ‚úÖ Pydantic model for request validation
class ChatRequest(BaseModel):
    query: str

@app.post("/chatbot")
async def chatbot_query(request: ChatRequest):
    print(f"üì© Received query: {request.query}")

    user_query = request.query.strip()
    if not user_query:
        raise HTTPException(status_code=400, detail="‚ùå Query cannot be empty.")

    try:
        retrieved_text = search_faiss(user_query)

        # ‚úÖ Custom prompt for better responses
        prompt = f"""
        You are an AI assistant named **BlockBot**, trained on the BlockShare documentation.
        Your job is to answer user queries based strictly on the provided documentation.If the answer is not in the document, try your knowledge to answer it.

        
        üîπ **User Question:** {user_query}  
        üîπ **Reference Document:**  
        {retrieved_text}  
        
        Ensure bold text appears as `**bold text**`, lists are formatted properly, and responses are concise.
        Now, generate a well-structured and accurate response in Markdown format.
        """

        # ‚úÖ Use the correct Gemini model
        model = genai.GenerativeModel("gemini-1.5-pro")

        # ‚úÖ Ensure response is formatted in HTML for proper display
        response = model.generate_content(prompt)

        

        return {"answer": response.text}
    
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))



# ‚úÖ Run with: uvicorn rag_chatbot:app --reload
